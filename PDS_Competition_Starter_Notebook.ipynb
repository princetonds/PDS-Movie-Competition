{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "PDS Competition Starter Notebook.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/princetonds/PDS-Movie-Competition/blob/main/PDS_Competition_Starter_Notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B53zT-souftI"
      },
      "source": [
        "# Welcome to the PDS Data Bowl!\n",
        "### An introductory workshop to Princeton's first-ever data science competition\n",
        "Authors: Joyce Luo (joyceluo@princeton.edu) & Nab Kar (nkar@princeton.edu)\n",
        "\n",
        "This workshop aims to:\n",
        "\n",
        "* help attendees get familiar with the competition format\n",
        "*introduce the dataset and task\n",
        "* get started with building their models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qObmmPgtxXdr"
      },
      "source": [
        "## Competition logistics\n",
        "**Timeline**: The competition will run until May 1, at which point we will stop accepting submissions.\n",
        "\n",
        "**Group size**: Students can work individually or teams of two. Finding a partner is recommended!\n",
        "\n",
        "**Dataset**: The dataset consists of over 3,000 movies with pertinent information (“features”) about each movie. Some features are:\n",
        "\n",
        "* Movie Budget\n",
        "* Genre(s)\n",
        "* Cast\n",
        "* Crew\n",
        "* Production company\n",
        "* Tagline for the movie (if exists)\n",
        "\n",
        "**Task**: Your task is to predict the movie’s revenue (in $) from all other features.\n",
        "\n",
        "We will provide train data (*movies_data_public_train.csv*) which contains all features including movie revenues. We will also provide testing data (*movies_data_test.csv*) which contains all features *except* revenues. Your job will be to submit predicted revenues for this test set on the [EvalAI competition page](https://eval.ai/web/challenges/challenge-page/871/overview).\n",
        "\n",
        "**Submission and Evaluation**:\n",
        "Each movie in the dataset is associated with an ID. We ask you to submit your predictions for the test set in a CSV file with the format:\n",
        "\n",
        "    MovieID, prediction\n",
        "    MovieID, prediction\n",
        "\n",
        "We’ll be evaluating your predictions using RMSLE, or Root Mean-Squared Log Error. This metric computes the squared difference between the log of your prediction and log of the actual value. Here’s the equation for RMSLE, where $\\hat{y}_i$ is your prediction and $y_i$ is the actual value:\n",
        "$$RMSLE = \\sqrt{\\frac{1}{n}\\sum_{i=1}^n \\left(\\log(\\hat{y}_i+1) - \\log(y_i+1)\\right)^2}$$\n",
        "\n",
        "*This next part is very important.* On the backend we’ve divided up the test set into a public and private test set. When you submit your predictions for the test set, you can either submit to the public or private leaderboard. Submitting to the public leaderboard allows you to see how you measure up to other participants, and you can submit up to ten times per day. However, your final score will be evaluated on the private leaderboard, for which you only have three submissions total. Do not forget to submit to the private leaderboard, but also use your submissions sparingly! The reason we do this is so that participants can’t “overfit” to the public leaderboard by constantly tweaking parameters of their model and resubmitting.\n",
        "\n",
        "More details can be found on the [EvalAI competition site](https://eval.ai/web/challenges/challenge-page/871/overview).\n",
        "\n",
        "**Prizes**: Prizes can be found at the [PDS competition page](https://princetonds.io/competition.html)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fcmJ2Z20YWb6"
      },
      "source": [
        "## Loading in the Data\n",
        "We provided a training and testing set that you should load in to your notebook--it can found on [GitHub](https://github.com/princetonds/PDS-Movie-Competition). You should also import necessary packages for data cleaning here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HmcYaqT-uKLU"
      },
      "source": [
        "!git clone https://github.com/princetonds/PDS-Movie-Competition"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "567VIpQKETWh"
      },
      "source": [
        "# imports important packages for data cleaning\n",
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F27u30yI3ShG"
      },
      "source": [
        "# loads in the training set and test set\n",
        "df_train = pd.read_csv(\"./PDS-Movie-Competition/movies_data_public_train.csv\")\n",
        "df_test = pd.read_csv(\"./PDS-Movie-Competition/movies_data_test.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AIaJme7IwGv5"
      },
      "source": [
        "Taking a quick look at the training data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gik6txRKwAuV"
      },
      "source": [
        "print(df_train.columns)\n",
        "df_train.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "namfdW4yxK1n"
      },
      "source": [
        "The testing data looks similar but lacks the *revenue* column."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tc5VMlAeYbdZ"
      },
      "source": [
        "## Cleaning the Data\n",
        "\n",
        "We provided some data cleaning examples to illustrate a possible way to clean the categorical data. Make sure to clean both the training and test set in the same way. We only cleaned certain columns, but if you want to use other columns, you should clean those as well as you see fit!\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lIaYEM0043SQ"
      },
      "source": [
        "df_train[\"cast\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tZVYsA1JETWm"
      },
      "source": [
        "### TRAINING CLEANING\n",
        "# gets first member of cast and puts in diff col\n",
        "cast_list = []\n",
        "# for each entry in the column \"cast\", get rid of excess characters\n",
        "# and split the long string of cast members into an array\n",
        "for i in range(len(df_train[\"cast\"])): \n",
        "    cast = df_train[\"cast\"].iloc[i]\n",
        "    cast = cast.replace(\"'\", \"\")\n",
        "    cast = cast.replace('\"', \"\")\n",
        "    cast = cast.replace(\"[\", \"\")\n",
        "    cast = cast.replace(\"]\", \"\")\n",
        "    cast = cast.split(',')\n",
        "    # get only the first member of the cast list\n",
        "    cast_list.append(cast[0])\n",
        "\n",
        "# put top cast members in new column\n",
        "df_train[\"main_cast\"] = cast_list\n",
        "\n",
        "# Note: if you want to completely clean the cast column, you will need to do \n",
        "# some cleaning similar to \"genres\" show below"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SKF3QRyF3IcI"
      },
      "source": [
        "df_train[\"main_cast\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kS4tC8v15Rjt"
      },
      "source": [
        "We clean other textual features similarly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "44hQYe6n5MXJ"
      },
      "source": [
        "# gets director (fist member of cast) and puts in diff col\n",
        "crew_list = []\n",
        "# for each entry in the column \"crew\", get rid of excess characters\n",
        "# and split the long string of crew members into an array\n",
        "for i in range(len(df_train[\"crew\"])): \n",
        "    crew = df_train[\"crew\"].iloc[i]\n",
        "    crew = crew.replace(\"'\", \"\") \n",
        "    crew = crew.replace('\"', \"\")\n",
        "    crew = crew.replace(\"[\", \"\")\n",
        "    crew = crew.replace(\"]\", \"\")\n",
        "    crew = crew.split(',')\n",
        "    # get only first member of crew list, which is director\n",
        "    crew_list.append(crew[0])\n",
        "# put director in new column\n",
        "df_train[\"director\"] = crew_list\n",
        "\n",
        "# cleans genres \n",
        "genres_list = []\n",
        "# for each entry in the column \"genres\", get rid of excess characters\n",
        "# and split the long string of genres into an array\n",
        "for i in range(len(df_train[\"genres\"])): \n",
        "    genres = df_train[\"genres\"].iloc[i]\n",
        "    genres = genres.replace(\"'\", \"\") \n",
        "    genres = genres.replace(\"[\", \"\")\n",
        "    genres = genres.replace(\"]\", \"\")\n",
        "    genres = genres.split(',')\n",
        "    # for each genre in the list, strip white spaces and add to new array\n",
        "    array = []\n",
        "    for gr in genres:\n",
        "        gr = gr.strip()\n",
        "        array.append(gr)\n",
        "    genres_list.append(array) \n",
        "# replace \"genres\" column with cleaned list\n",
        "df_train[\"genres\"] = genres_list\n",
        "\n",
        "# cleans production companies\n",
        "comp_list = []\n",
        "# for each entry in the column \"production_companies\", get rid of excess \n",
        "# characters and split the long string of companies into an array\n",
        "for i in range(len(df_train[\"production_companies\"])): \n",
        "    comp = df_train[\"production_companies\"].iloc[i]\n",
        "    comp = comp.replace(\"'\", \"\") \n",
        "    comp = comp.replace(\"[\", \"\")\n",
        "    comp = comp.replace(\"]\", \"\")\n",
        "    comp = comp.split(',')\n",
        "    # for each company in the list, strip white spaces and add to new array\n",
        "    array = []\n",
        "    for idx, co in enumerate(comp):\n",
        "        if (idx < 5):\n",
        "          co = co.strip()\n",
        "          array.append(co)\n",
        "    comp_list.append(array) \n",
        "# replace \"production_companies\" column with cleaned list\n",
        "df_train[\"production_companies\"] = comp_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ObEm5Aqn5r2T"
      },
      "source": [
        "We also clean the testing data in a similar manner."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "An3IdLj25ur2"
      },
      "source": [
        "### TESTING CLEANING \n",
        "# gets first member of cast and puts in diff col\n",
        "cast_list = []\n",
        "# for each entry in the column \"cast\", get rid of excess characters\n",
        "# and split the long string of cast members into an array\n",
        "for i in range(len(df_test[\"cast\"])): \n",
        "    cast = df_test[\"cast\"].iloc[i]\n",
        "    cast = cast.replace(\"'\", \"\")\n",
        "    cast = cast.replace('\"', \"\")\n",
        "    cast = cast.replace(\"[\", \"\")\n",
        "    cast = cast.replace(\"]\", \"\")\n",
        "    cast = cast.split(',')\n",
        "    # get only the first member of the cast list\n",
        "    cast_list.append(cast[0])\n",
        "# put top cast members in new column\n",
        "df_test[\"main_cast\"] = cast_list\n",
        "# Note: if you want to completely clean the cast column, you will need to do \n",
        "# some cleaning similar to \"genres\" show below\n",
        "\n",
        "# gets director and puts in diff col\n",
        "crew_list = []\n",
        "# for each entry in the column \"crew\", get rid of excess characters\n",
        "# and split the long string of crew members into an array\n",
        "for i in range(len(df_test[\"crew\"])): \n",
        "    crew = df_test[\"crew\"].iloc[i]\n",
        "    crew = crew.replace(\"'\", \"\") \n",
        "    crew = crew.replace('\"', \"\")\n",
        "    crew = crew.replace(\"[\", \"\")\n",
        "    crew = crew.replace(\"]\", \"\")\n",
        "    crew = crew.split(',')\n",
        "    # get only first member of crew list, which is director\n",
        "    crew_list.append(crew[0])\n",
        "# put director in new column\n",
        "df_test[\"director\"] = crew_list\n",
        "\n",
        "# cleans genres \n",
        "genres_list = []\n",
        "# for each entry in the column \"genres\", get rid of excess characters\n",
        "# and split the long string of genres into an array\n",
        "for i in range(len(df_test[\"genres\"])): \n",
        "    genres = df_test[\"genres\"].iloc[i]\n",
        "    genres = genres.replace(\"'\", \"\") \n",
        "    genres = genres.replace(\"[\", \"\")\n",
        "    genres = genres.replace(\"]\", \"\")\n",
        "    genres = genres.split(',')\n",
        "    # for each genre in the list, strip white spaces and add to new array\n",
        "    array = []\n",
        "    for gr in genres:\n",
        "        gr = gr.strip()\n",
        "        array.append(gr)\n",
        "    genres_list.append(array) \n",
        "# replace \"genres\" column with cleaned list\n",
        "df_test[\"genres\"] = genres_list\n",
        "\n",
        "# cleans production companies\n",
        "comp_list = []\n",
        "# for each entry in the column \"production_companies\", get rid of excess \n",
        "# characters and split the long string of companies into an array\n",
        "for i in range(len(df_test[\"production_companies\"])): \n",
        "    comp = df_test[\"production_companies\"].iloc[i]\n",
        "    comp = comp.replace(\"'\", \"\") \n",
        "    comp = comp.replace(\"[\", \"\")\n",
        "    comp = comp.replace(\"]\", \"\")\n",
        "    comp = comp.split(',')\n",
        "    # for each company in the list, strip white spaces and add to new array\n",
        "    array = []\n",
        "    for idx, co in enumerate(comp):\n",
        "        if (idx < 5):\n",
        "          co = co.strip()\n",
        "          array.append(co)\n",
        "    comp_list.append(array) \n",
        "# replace \"production_companies\" column with cleaned list\n",
        "df_test[\"production_companies\"] = comp_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8313cHRJY7Ij"
      },
      "source": [
        "## Basic Exploratory Data Analysis\n",
        "Here we have done some basic EDA, so you can visualize the data and make decisions about your models based on this EDA. Feel free to do more EDA to visualize other variables. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DzxR8hj6_c-H"
      },
      "source": [
        "# imports data visualization packages\n",
        "import seaborn\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FydAASl28QEb"
      },
      "source": [
        "df_train.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H-LYZ1pc6PHD"
      },
      "source": [
        "seaborn.histplot(df_train[\"budget\"])\n",
        "plt.xlabel('revenue')\n",
        "plt.ylabel('number of movies')\n",
        "plt.title('Histogram of Revenue and Movies')\n",
        "plt.show()\n",
        "\n",
        "seaborn.histplot(df_train[\"runtime\"])\n",
        "plt.xlabel('runtime')\n",
        "plt.ylabel('number of movies')\n",
        "plt.title('Histogram of Runtime and Movies')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u2AFNGwh6_Hd"
      },
      "source": [
        "arr2 = df_train[\"genres\"].values\n",
        "from collections import Counter\n",
        "c = Counter()\n",
        "for xs in arr2:\n",
        "    for x in set(xs):\n",
        "        c[x] += 1\n",
        "   \n",
        "categories = []\n",
        "vals = []\n",
        "for i in c:\n",
        "    categories.append(i)\n",
        "    vals.append(c[i])\n",
        "\n",
        "fig = plt.figure()\n",
        "ax = fig.add_axes([0,0,1,1])\n",
        "ax.bar(categories,vals, label='genres')\n",
        "ax.set_title('Distribution of Genres across Movies')\n",
        "plt.xticks(rotation = 90)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z4OBrdKo7ezw"
      },
      "source": [
        "seaborn.heatmap(df_train.drop(columns=['Unnamed: 0', 'movie_id']).corr(), annot=True, vmin=-1, vmax=1)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f89nDxxIZGKr"
      },
      "source": [
        "## Preparing certain textual variables to use in the model\n",
        "\n",
        "Many of the given features are textual. Here we illustrate a simple example of how the textual data could be used in your model to predict revenue. Specifically, we define two dummy variables for if a movie is directed by one of two high-profile directors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xqy01t84qZa2"
      },
      "source": [
        "df_train['MS_dummy'] = df_train['director'] == 'Martin Scorsese'\n",
        "df_train['SS_dummy'] = df_train['director'] == 'Steven Spielberg'\n",
        "\n",
        "df_test['MS_dummy'] = df_test['director'] == 'Martin Scorsese'\n",
        "df_test['SS_dummy'] = df_test['director'] == 'Steven Spielberg'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BlKEMBlykdtG"
      },
      "source": [
        "df_train['MS_dummy'].sum(), df_train['SS_dummy'].sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2d3LR7nqkm8P"
      },
      "source": [
        "df_test['MS_dummy'].sum(), df_test['SS_dummy'].sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qkhBXnO6kvfH"
      },
      "source": [
        "We will evaluate two models, one which does not include these dummy variables and one which does."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HRqu9UdtZiXT"
      },
      "source": [
        "## Creating and evaluating models\n",
        "\n",
        "Here we provide a very simple example for modeling revenue. In order to analyze model performance, we suggest creating an additional validation set from the training data. We evaluate how good the model is based on the root mean squared log error (RMSLE) and $R^2$ value. If you predict negative numbers, you should threshold your predictions at 0 before calculating the RMSLE or you will get an error. When we evaluate your predictions, we will also be thresholding at 0, so it is okay to have negative values in your submission. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VsOrz4PSETWo"
      },
      "source": [
        "# imports necessary packages for modeling\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_log_error, r2_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k2zavX0IdOl8"
      },
      "source": [
        "#### Without textual dummy variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Om9Ge_7eJCyp"
      },
      "source": [
        "# creates your feature dataframe and outcome column\n",
        "X = df_train[[\"budget\", \"runtime\", \"popularity\"]]\n",
        "y = df_train[\"revenue\"]\n",
        "\n",
        "# creates a training and validation set from the overall training data (20% validation)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=0)\n",
        "\n",
        "# standardize features\n",
        "scalar = StandardScaler().fit(X_train)\n",
        "X_train = scalar.transform(X_train)\n",
        "X_val = scalar.transform(X_val)\n",
        "\n",
        "# creates a random forest\n",
        "rf = RandomForestRegressor(max_depth=7, random_state=0)\n",
        "\n",
        "# fits the model\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "# predicts outcomes for the training data\n",
        "y_train_pred = rf.predict(X_train)\n",
        "\n",
        "# calculates the RMSLE and R^2 for the training data\n",
        "rf_train_rmsle = np.sqrt(mean_squared_log_error(y_train, y_train_pred))\n",
        "rf_train_r2 = r2_score(y_train, y_train_pred)\n",
        "print(\"RF training: RMSLE \\t= %f & R^2 \\t= %f\" % (rf_train_rmsle, rf_train_r2))\n",
        "\n",
        "# predicts outcomes for the validation data (this is important to see how your\n",
        "# model extends to unseen data)\n",
        "y_val_pred = rf.predict(X_val)\n",
        "\n",
        "# calculates the RMSLE and R^2 for the testing data\n",
        "rf_val_rmsle = np.sqrt(mean_squared_log_error(y_val, y_val_pred))\n",
        "rf_val_r2 = r2_score(y_val, y_val_pred)\n",
        "print(\"RF validation: RMSLE \\t= %f & R^2 \\t= %f\" % (rf_val_rmsle, rf_val_r2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KkQmbrT-dMAE"
      },
      "source": [
        "#### With textual dummy variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XncHN89ILY_l"
      },
      "source": [
        "# creates your feature dataframe and outcome column\n",
        "X = df_train[[\"budget\", \"runtime\", \"popularity\", \"MS_dummy\", \"SS_dummy\"]]\n",
        "y = df_train[\"revenue\"]\n",
        "\n",
        "# creates a training and validation set from the overall training data (20% validation)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=0)\n",
        "\n",
        "# standardize features\n",
        "scalar = StandardScaler().fit(X_train)\n",
        "X_train = scalar.transform(X_train)\n",
        "X_val = scalar.transform(X_val)\n",
        "\n",
        "# creates a random forest\n",
        "rf2 = RandomForestRegressor(max_depth=7, random_state=0)\n",
        "\n",
        "# fits the model\n",
        "rf2.fit(X_train, y_train)\n",
        "\n",
        "# predicts outcomes for the training data\n",
        "y_train_pred = rf2.predict(X_train)\n",
        "\n",
        "# calculates the RMSLE and R^2 for the training data\n",
        "rf2_train_rmsle = np.sqrt(mean_squared_log_error(y_train, y_train_pred))\n",
        "rf2_train_r2 = r2_score(y_train, y_train_pred)\n",
        "print(\"RF2 training: RMSLE \\t= %f & R^2 = %f\" % (rf2_train_rmsle, rf2_train_r2))\n",
        "\n",
        "# predicts outcomes for the testing data (this is important to see how your\n",
        "# model extends to unseen data)\n",
        "y_val_pred = rf2.predict(X_val)\n",
        "\n",
        "# calculates the RMSLE and R^2 for the testing data\n",
        "rf2_val_rmsle = np.sqrt(mean_squared_log_error(y_val, y_val_pred))\n",
        "rf2_val_r2 = r2_score(y_val, y_val_pred)\n",
        "print(\"RF2 validation: RMSLE \\t= %f & R^2 = %f\" % (rf2_val_rmsle, rf2_val_r2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AgZAF1JUl8BV"
      },
      "source": [
        "It appears that we can squeeze out marginal performance improvements by using the textual data! In fact, we suspect that the best models will use the textual features in creative ways to maximize performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vhlDHmPjFim8"
      },
      "source": [
        "#### Make sure to train on the entire training + validation set\n",
        "Since the model was developed on a subset of the training data, you need to use all of the official training data to train the model that is submitted to the leaderboard."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WqjUMJIHFxAV"
      },
      "source": [
        "# train new model on all of the training data that was provided\n",
        "X = df_train[[\"budget\", \"runtime\", \"popularity\", \"MS_dummy\", \"SS_dummy\"]]\n",
        "y = df_train[\"revenue\"]\n",
        "\n",
        "rf_final = RandomForestRegressor(max_depth=7, random_state=0)\n",
        "rf_final.fit(X, y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V5UBNYCPRn5C"
      },
      "source": [
        "## How to upload your output to EvalAI\n",
        "The required submission format is a .csv file with two columns, *movie_id* and *predicted revenue*. You should submit a file like this when submitting to both the Public phase and Private phase of the competition on EvalAI."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kzGw-00ARnOP"
      },
      "source": [
        "# generate predictions on the official provided test set\n",
        "X_test = df_test[[\"budget\", \"runtime\",\"popularity\", \"MS_dummy\", \"SS_dummy\"]]\n",
        "\n",
        "# predict on the testing data\n",
        "y_test_pred = rf_final.predict(X_test)\n",
        "\n",
        "# get the movie ids and corresponding revenues you predicted and put in a new dataframe\n",
        "results = df_test[\"movie_id\"].copy()\n",
        "results = pd.DataFrame(results)\n",
        "results[\"predicted revenue\"] = y_test_pred \n",
        "\n",
        "# convert dataframe into csv that you will submit to the leaderboard\n",
        "results.to_csv(r'test_competition_entry.csv', index = False, header=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jFW1OGl7BnSE"
      },
      "source": [
        "results"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rkM-8WnlnykT"
      },
      "source": [
        "You can now upload these predictions to EvalAI to see how well your model performs against other models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ZuyVLEKo3iZ"
      },
      "source": [
        "## Good luck!"
      ]
    }
  ]
}